1. Design POI(point of interest)



2. Design Instagram




3. Design memcached



4. Design display top 10 hits music




5. OOD: design a vending machine.




6. web server: how to handle request, TCP connection limitation,
   if TCP were turned off, why server were not shutdown immediately




7. Design server to host music, how to download music, update music, recommend music




8. Design Rate limiter
https://github.com/google/guava/blob/9b94fb3965c6869b0ac47420958a4bbae0b2d54c/android/guava/src/com/google/common/util/concurrent/SmoothRateLimiter.java


9. Design Hit Counter
http://blog.gainlo.co/index.php/2016/09/12/dropbox-interview-design-hit-counter/?utm_source=glassdoor&utm_campaign=glassdoor&utm_medium=92116
It starts with a simple question – if you are building a website, how do you count the number of visitors for the past 1 minute?


10. Design a simplified version of Twitter where people can post tweets, follow other people and favorite tweets.

Q1: First of all, how many users do we expect this system to handle?
A1: “well… to make things interesting, let’s aim for 10 million users generating around 100 million requests per day”


Q2: how connected will these users be?(they will form a graph with the users being the nodes and the edges will represent who follows whom. )
A2: “we expect that each user will be following 200 other users on average,
but expect some extraordinary users with tens of thousands of followers”


Q3: We have another dimension in the application: tweets.
Producing new tweets and favoriting them should be the most common write operations in the application.
But how many requests will that generate?
A3: “Hm, that’s hard to say… we expect that there will be a maximum of 10 million tweets per day and
each tweet will probably be favorited twice on average but again, expect some big outliers.”


Let’s make a few very simple calculations with the information we just received.
We will have around 10 million users. Average number of followed other users is 200.
This means that the network of users will have about 200 * 10 million edges. This makes 2 billion edges.
If the average number of tweets per day is 10 million the number of favorites will then be 20 million.


As it is often the case, we can divide our architecture in two logical parts:
1) the logic, which will handle all incoming requests to the application and
2) the data storage that we will use to store all the data that needs to be persisted.
If you learn enough about popular software systems, especially the ones that are accessed
through a web browser you will notice that such a breakdown is quite popular and natural.

 our application will need to handle requests for:

    posting new tweets(write)
    following a user(write)
    favoriting a tweet(write)
    displaying data about users and tweets(read)


Handling user requests ?
We know that the expected daily load is 100 million requests.
This means that on average the app will receive around 1150 requests per second.




Load balancing
across multiple application instances is a commonly used technique for optimizing resource utilization,
maximizing throughput, reducing latency, and ensuring fault-tolerant configurations.


The following load balancing mechanisms (or methods) are supported in nginx:

    round-robin — requests to the application servers are distributed in a round-robin fashion,
    least-connected — next request is assigned to the server with the least number of active connections,
    ip-hash — a hash-function is used to determine what server should be selected for the next request
             (based on the client’s IP address).


Session persistence

Please note that with round-robin or least-connected load balancing, each subsequent client’s request can be
potentially distributed to a different server. There is no guarantee that the same client will be always
directed to the same server.

If there is the need to tie a client to a particular application server — in other words, make the client’s
session “sticky” or “persistent” in terms of always trying to select a particular server —
the ip-hash load balancing mechanism can be used.

With ip-hash, the client’s IP address is used as a hashing key to determine what server in a server group
should be selected for the client’s requests. This method ensures that the requests from the same client
will always be directed to the same server except when this server is unavailable.


 There are 3 important factors used to measure a load balancer's performance :

    The session rate(depends on CPU)(request goes in)
    This factor is very important, because it directly determines when the load balancer will not be
    able to distribute all the requests it receives. It is mostly dependant on the CPU. Sometimes,
    you will hear about requests/s or hits/s, and they are the same as sessions/s in HTTP/1.0 or HTTP/1.1
    with keep-alive disabled. Requests/s with keep-alive enabled is generally much higher (since it significantly
    reduces system-side work) but is often meaningless for internet-facing deployments since clients
    often open a large amount of connections and do not send many requests per connection on average.
    This factor is measured with varying object sizes, the fastest results generally coming from
    empty objects (eg: HTTP 302, 304 or 404 response codes). Session rates around 100,000 sessions/s
    can be achieved on Xeon E5 systems in 2014.

    The session concurrency(depends on memory)
    This factor is tied to the previous one. Generally, the session rate will drop when the number of
    concurrent sessions increases (except with the epoll or kqueue polling mechanisms). The slower the servers,
    the higher the number of concurrent sessions for a same session rate. If a load balancer receives 10000 sessions
    per second and the servers respond in 100 ms, then the load balancer will have 1000 concurrent sessions.
    This number is limited by the amount of memory and the amount of file-descriptors the system can handle.
    With 16 kB buffers, HAProxy will need about 34 kB per session, which results in around 30000 sessions
    per GB of RAM. In practise, socket buffers in the system also need some memory and 20000 sessions per GB of RAM
    is more reasonable. Layer 4 load balancers generally announce millions of simultaneous sessions
    because they need to deal with the TIME_WAIT sockets that the system handles for free in a proxy.
    Also they don't process any data so they don't need any buffer. Moreover, they are sometimes designed
    to be used in Direct Server Return mode, in which the load balancer only sees forward traffic,
    and which forces it to keep the sessions for a long time after their end to avoid cutting sessions
    before they are closed.

    The data forwarding rate(depend on CPU)(data going out)
    This factor generally is at the opposite of the session rate. It is measured in Megabytes/s (MB/s),
    or sometimes in Gigabits/s (Gbps). Highest data rates are achieved with large objects to minimise the overhead
    caused by session setup and teardown. Large objects generally increase session concurrency, and high session
    concurrency with high data rate requires large amounts of memory to support large windows. High data rates
    burn a lot of CPU and bus cycles on software load balancers because the data has to be copied from the input
    interface to memory and then back to the output device. Hardware load balancers tend to directly switch packets
    from input port to output port for higher data rate, but cannot process them and sometimes fail to touch a header
    or a cookie. Haproxy on a typical Xeon E5 of 2014 can forward data up to about 40 Gbps. A fanless 1.6 GHz Atom CPU
    is slightly above 1 Gbps.


A load balancer's performance related to these factors is generally announced for the best case (eg: empty objects
for session rate, large objects for data rate). This is not because of lack of honesty from the vendors, but because
it is not possible to tell exactly how it will behave in every combination. So when those 3 limits are known,
the customer should be aware that it will generally perform below all of them. A good rule of thumb on software
load balancers is to consider an average practical performance of half of maximal session and data rates for
average sized objects.


Storing the data
We need to store data about our users and their tweets to make the application complete.

After this quick analysis it is obvious that the tweets will take up the majority of our storage’s space.
In general, you don’t need to make very detailed calculations especially if you don’t have much time. However,
it is important to build a rough idea about the size of the data that you will need to handle. If you don’t figure
that out any design decision at the higher or lower level may be inappropriate.


We know that the expected size of the data to store is around 2.6 - 2.7 terabytes.
Real-life examples show that famous companies like Twitter and Facebook manage to use relational databases
for handling much bigger loads than that.



Twitter's original tweet store:

    Temporally sharded tweets was a good-idea-at-the-time architecture.

    Temporal sharding simply means tweets from the same date range are stored together on the same shard.

    The problem is tweets filled up one machine, then a second, and then a third.
    You end up filling up one machine after another.

    This is a pretty common approach and one that has some real flaws:
        Load balancing. Most of the old machines didn't get any traffic because people are interested in
        what is happening now, especially with Twitter.

        Expensive. They filled up one machine, with all its replication slaves, every three weeks,
        which is an expensive setup.

        Logistically complicated. Building a whole new cluster every three weeks is a pain for the DBA team.



In order to handle the incoming read requests we may need to use a caching solution, which stands in front of
the database server. One such popular tool is memcached. It could save us a lot of reads directly from the database.


For example, you just mentioned using a caching solution and imagine that the interviewer asks you:

    “Sounds good, but could you tell me more about why reading from the cache would be better than
    just reading from the database?”

To answer the question asked, we could say that a database stores data on disk and it is much slower to read
from disk than from memory. A solution like memcached stores data in memory, which provides way faster access.
We would need to clarify further that databases usually have their own caching mechanisms but with memcached
we have better control over what gets cached and how. For example, we could store more complex pieces of data
like the results from popular queries.


Going further, in order to make it possible to answer read queries fast we will definitely need to
add the appropriate indexes. This will also be vital for executing quick queries joining tables.
Considering the size of the data we may also think about partitioning the data in some way.
This can improve the read and write speeds and also make administration tasks like backups faster.



Database schema
“If you’re going to use a relational database for storing all the data
could you draft the tables and the relations between them?”


These two entities have several types of relations between them:

    users create tweets
    users can follow users
    users favorite tweets


he first relation is addressed by sticking the user ID to each tweet.
This is possible because each tweet is created by exactly one user.
It’s a bit more complicated when it comes to following users and favoriting tweets.
The relationship there is many-to-many.




Table connections

    ID of user that follows (follower_id)
    ID of user that is followed (followee_id)
    date of creation (created_at)



Table favorites

    ID of user that favorited (user_id)
    ID of favorited tweet (tweet_id)
    date of creation (created_at)



Let’s assume that the queries to our data will be filtering users by their username.
If that’s the case we will definitely want to build an index over this field to optimize the times for such queries.


The next popular query will fetch tweets for a given user.
The query needed for doing that will filter tweets using user_id, which every tweet has.
It makes a lot of sense to build an index over this field in the tweets table,
so that such queries are performed quickly.

We will probably not want to fetch all tweets of a user at once. For example, if a given user has accumulated
several thousand tweets over time, on their profile page we will start by showing the most recent 20 or something
like that. This means that we could use a query, which not only filters by user_id but also orders by creation date
(created_at) and limits the result. Based on that we may think about expanding our index to include the user_id column
but to also include the created_at column. When we have an index over more than one column the order of the columns
matters. If our index looks like that: <user_id, created_at>, making a query filtering by just user_id will
take advantage of the index even though we are not filtering by the second column. So, such an index will allow us
to filter either by just user_id, or by both columns. This will allow us to fetch all tweets authored by a given user
or to isolate just the tweets created in a given time frame. Both will be useful queries for our application.


Having discussed these base use cases, our interviewer suggests that you think about one more possible situation:
“Do you think you could support with our database design the ability to display a page for a given user
with their latest tweets that were favorited at least once?”



Building a RESTful API
Another thing that our interviewer could be interested in is how our front-end would “talk” to the back-end system.

Probably the most popular answer nowadays would be by exposing a RESTful API on the back-end side,
which has a few endpoints returning JSON objects as responses.


GET /api/users/<username>
GET /api/users/<username>/tweets
GET /api/users/<username>/tweets?page=4

GET /api/users/<username>/followers
GET /api/users/<username>/followees

Let’s look at creating new data. For example we will need an endpoint for posting a new tweet:
POST /api/users/<username>/tweets
POST /api/users/<username>/followers


GET /api/users/<username>/tweets/<tweet_id>/favorites
POST /api/users/<username>/tweets/<tweet_id>/favorites


Of course, we will need some sort of authentication to be put in place,
so that we make sure that not everyone can query our exposed API.





Additional Considerations

Increased number of read requests
We have our implementation of the system and it handles everything perfectly.
But what would happen if suddenly we got lucky and people started visiting our application 5 times more often
generating 5 times more read requests caused by viewing posts and user profiles.
What would be the first place that will most likely become a bottleneck?


One very natural answer is our database. It could become overwhelmed with all the read requests coming to it.

One typical way to handle more read requests would be to use replication. This way we could increase the number of
database instances that hold a copy of the data and allow the application to read this data.
Of course, this will help if the write requests are not increased dramatically.


An alternative approach could be to shard our database and spread the data across different machines.
This will help us if we need to write more data than before and the database cannot handle it.
Such an approach does not come for free and it involves other complications that we would need to take care of.
Consider getting familiar with these popular techniques for scaling a relational database.


If we manage to stabilize our database, another point where we could expect problems is the web application itself.
If we’ve been running it on a limited set of machines,
which cannot handle all the load anymore this could lead to slow response times.
One good thing about our high-level design is that it allows us to just add more machines running the application.


This way our load balancer could become a single point of failure and a bottleneck if the number of requests is
really high. In such cases we could start thinking about doing additional load balancing using
DNS and directing requests for our domain to different machines, which are acting as load balancers themselves.



Scaling the database

One approach mentioned above is to add an in-memory cache solution in front of the database with the goal to not
send repeated read requests to the database itself. We could use a key-value store like memcached to handle that.

But this could be insufficient if our data grows too quickly.
In such cases we may have to start thinking about partitioning this data and storing it on separate servers
to increase availability and spread the load.


Sharding our data could be a good and necessary approach in such cases. It must be planned and used with care
because it will cause additional complications. But sometimes it is just required.
It is highly recommended that you read more about this topic and all the implication it brings along.
This will help you justify your decision to shard and to have a discussion about the possible downsides of doing it.



Unexpected traffic
In the beginning the interviewer warned us that there will be outliers in the data.
This means that some users will have many more followers than the average.
Also, some tweets will attract a lot of attention during a short period of time.
In most cases such outliers will generate peaks in the number of requests that our application receives.


As we mentioned earlier this could increase the load on our database.
In such a situation using a caching solution could help a lot.
The idea is that it will be able to answer the popular and repeating requests coming from the application
and these requests will never touch the database, which will be busy replying to other queries.


We could also experience unusual peaks in the requests hitting our application servers.
If they are not enough to respond quickly enough to all requests this could cause timeout to occur for some of
the users. In such situations solutions that offer auto-scaling of the available computing nodes could save the day.
Some companies offering such services are Amazon and Heroku and they were already mentioned in this example.



11. Design the summarization problem
"In our company we already have developed a great library that can be used to summarize text articles.
Just feed it the whole text and it will return a decent summary that is just a few sentences long.

We need to put this in production and make it scalable.
We expect that our customers will submit text articles from our mobile app and also from our website.

The library currently takes between 0.1 and 5 seconds to summarize an article.
You need to design a system that uses our existing library and allows users to submit text articles
through the mobile app and through the website.

We anticipate that this service will be used around 1 million times a month.
Our desire is to not respond in more than 10 seconds to each request."














Picking the right architecture = Picking the right battles + Managing trade-offs

Basic Steps

    Clarify and agree on the scope of the system

    User cases (description of sequences of events that, taken together, lead to a system doing something useful)
        Who is going to use it?
        How are they going to use it?
    Constraints
        Mainly identify traffic and data handling constraints at scale.
        Scale of the system such as requests per second, requests types, data written per second, data read per second)
        Special system requirements such as multi-threading, read or write oriented.

    High level architecture design (Abstract design)

    Sketch the important components and connections between them, but don't go into some details.
        Application service layer (serves the requests)
        List different services required.
        Data Storage layer
        eg. Usually a scalable system includes webserver (load balancer), service (service partition),
        database (master/slave database cluster) and caching systems.

    Component Design

    Component + specific APIs required for each of them.
    Object oriented design for functionalities.
        Map features to modules: One scenario for one module.
        Consider the relationships among modules:
            Certain functions must have unique instance (Singletons)
            Core object can be made up of many other objects (composition).
            One object is another object (inheritance)
    Database schema design.

    Understanding Bottlenecks

    Perhaps your system needs a load balancer and many machines behind it to handle the user requests.
    * Or maybe the data is so huge that you need to distribute your database on multiple machines.
    What are some of the downsides that occur from doing that?

    Is the database too slow and does it need some in-memory caching?

    Scaling your abstract design

    Vertical scaling
        You scale by adding more power (CPU, RAM) to your existing machine.
    Horizontal scaling
        You scale by adding more machines into your pool of resources.
    Caching
        Load balancing helps you scale horizontally across an ever-increasing number of servers,
        but caching will enable you to make vastly better use of the resources you already have,
        as well as making otherwise unattainable product requirements feasible.

        Application caching requires explicit integration in the application code itself.
        Usually it will check if a value is in the cache; if not, retrieve the value from the database.

        Database caching tends to be "free". When you flip your database on, you're going to get some level of default
        configuration which will provide some degree of caching and performance.
        Those initial settings will be optimized for a generic usecase, and by tweaking them to your system's
        access patterns you can generally squeeze a great deal of performance improvement.

        In-memory caches are most potent in terms of raw performance. This is because they store their
        entire set of data in memory and accesses to RAM are orders of magnitude faster
        than those to disk. eg. Memcached or Redis.
        eg. Precalculating results (e.g. the number of visits from each referring domain for the previous day),
        eg. Pre-generating expensive indexes (e.g. suggested stories based on a user's click history)
        eg. Storing copies of frequently accessed data in a faster backend (e.g. Memcache instead of PostgreSQL.
    Load balancing
        Public servers of a scalable web service are hidden behind a load balancer.
        This load balancer evenly distributes load (requests from your users)
        onto your group/cluster of application servers.

        Types: Smart client (hard to get it perfect), Hardware load balancers ($$$ but reliable),
        Software load balancers (hybrid - works for most systems)

Load Balancing

    Database replication
        Database replication is the frequent electronic copying data from a database in one computer or server
        to a database in another so that all users share the same level of information.
        The result is a distributed database in which users can access data relevant to their tasks without
        interfering with the work of others. The implementation of database replication for the purpose
        of eliminating data ambiguity or inconsistency among users is known as normalization.
    Database partitioning
        Partitioning of relational data usually refers to decomposing your tables
        either row-wise (horizontally) or column-wise (vertically).
    Map-Reduce
        For sufficiently small systems you can often get away with adhoc queries on a SQL database,
        but that approach may not scale up trivially once the quantity of data stored or write-load
        requires sharding your database, and will usually require dedicated slaves for the purpose of performing
        these queries (at which point, maybe you'd rather use a system designed for analyzing large quantities
        of data, rather than fighting your database).
        Adding a map-reduce layer makes it possible to perform data and/or processing intensive operations
        in a reasonable amount of time. You might use it for calculating suggested users in a social graph,
        or for generating analytics reports. eg. Hadoop, and maybe Hive or HBase.
    Platform Layer (Services)
        Separating the platform and web application allow you to scale the pieces independently.
        If you add a new API, you can add platform servers
        without adding unnecessary capacity for your web application tier.
        Adding a platform layer can be a way to reuse your infrastructure for multiple products or
        interfaces (a web application, an API, an iPhone app, etc) without writing too much redundant
        boilerplate code for dealing with caches, databases, etc.

Platform Layer
Key topics for designing a system

    Concurrency
    Do you understand threads, deadlock, and starvation? Do you know how to parallelize algorithms?
    Do you understand consistency and coherence?

    Networking
    Do you roughly understand IPC and TCP/IP? Do you know the difference between throughput and latency,
    and when each is the relevant factor?

    Abstraction
    You should understand the systems you’re building upon. Do you know roughly how an OS, file system,
    and database work? Do you know about the various levels of caching in a modern OS?

    Real-World Performance
    You should be familiar with the speed of everything your computer can do,
    including the relative performance of RAM, disk, SSD and your network.

    Estimation
    Estimation, especially in the form of a back-of-the-envelope calculation,
    is important because it helps you narrow down the list of possible solutions to only the ones that are feasible.
    Then you have only a few prototypes or micro-benchmarks to write.

    Availability & Reliability
    Are you thinking about how things can fail, especially in a distributed environment?
    Do know how to design a system to cope with network failures? Do you understand durability?


Web App System design considerations:

    Security (CORS)
    Using CDN
        A content delivery network (CDN) is a system of distributed servers (network) that deliver webpages
        and other Web content to a user based on the geographic locations of the user,
        the origin of the webpage and a content delivery server.
        This service is effective in speeding the delivery of content of websites with high traffic
        and websites that have global reach. The closer the CDN server is to the user geographically,
        the faster the content will be delivered to the user.
        CDNs also provide protection from large surges in traffic.
    Full Text Search
        Using Sphinx/Lucene/Solr - which achieve fast search responses because, instead of searching the text directly,
        it searches an index instead.
    Offline support/Progressive enhancement
        Service Workers
    Web Workers
    Server Side rendering
    Asynchronous loading of assets (Lazy load items)
    Minimizing netwrok requests (Http2 + bundling/sprites etc)
    Developer productivity/Tooling
    Accessibility
    Internationalization
    Responsive design
    Browser compatibility

Working Components of Front-end Architecture

    Code
        HTML5/WAI-ARIA
        CSS/Sass Code standards and organization
        Object-Oriented approach (how do objects break down and get put together)
        JS frameworks/organization/performance optimization techniques
        Asset Delivery - Front-end Ops
    Documentation
        Onboarding Docs
        Styleguide/Pattern Library
        Architecture Diagrams (code flow, tool chain)
    Testing
        Performance Testing
        Visual Regression
        Unit Testing
        End-to-End Testing
    Process
        Git Workflow
        Dependency Management (npm, Bundler, Bower)
        Build Systems (Grunt/Gulp)
        Deploy Process
        Continuous Integration (Travis CI, Jenkins)



Links
How to rock a systems design interview

System Design Interviewing

Scalability for Dummies

Introduction to Architecting Systems for Scale

Scalable System Design Patterns

Scalable Web Architecture and Distributed Systems

What is the best way to design a web site to be highly scalable?

How web works?


https://gist.github.com/zerghua/941fec17c95547ef15af16bb7abd88dd