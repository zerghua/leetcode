1. Design POI(point of interest)



2. Design Instagram




3. Design memcached



4. Design display top 10 hits music




5. OOD: design a vending machine.




6. web server: how to handle request, TCP connection limitation,
   if TCP were turned off, why server were not shutdown immediately




7. Design server to host music, how to download music, update music, recommend music




8. Design Rate limiter
https://github.com/google/guava/blob/9b94fb3965c6869b0ac47420958a4bbae0b2d54c/android/guava/src/com/google/common/util/concurrent/SmoothRateLimiter.java


9. Design Hit Counter
http://blog.gainlo.co/index.php/2016/09/12/dropbox-interview-design-hit-counter/?utm_source=glassdoor&utm_campaign=glassdoor&utm_medium=92116
It starts with a simple question – if you are building a website, how do you count the number of visitors for the past 1 minute?


10. Design a simplified version of Twitter where people can post tweets, follow other people and favorite tweets.

Q1: First of all, how many users do we expect this system to handle?
A1: “well… to make things interesting, let’s aim for 10 million users generating around 100 million requests per day”


Q2: how connected will these users be?(they will form a graph with the users being the nodes and the edges will represent who follows whom. )
A2: “we expect that each user will be following 200 other users on average,
but expect some extraordinary users with tens of thousands of followers”


Q3: We have another dimension in the application: tweets.
Producing new tweets and favoriting them should be the most common write operations in the application.
But how many requests will that generate?
A3: “Hm, that’s hard to say… we expect that there will be a maximum of 10 million tweets per day and
each tweet will probably be favorited twice on average but again, expect some big outliers.”


Let’s make a few very simple calculations with the information we just received.
We will have around 10 million users. Average number of followed other users is 200.
This means that the network of users will have about 200 * 10 million edges. This makes 2 billion edges.
If the average number of tweets per day is 10 million the number of favorites will then be 20 million.


As it is often the case, we can divide our architecture in two logical parts:
1) the logic, which will handle all incoming requests to the application and
2) the data storage that we will use to store all the data that needs to be persisted.
If you learn enough about popular software systems, especially the ones that are accessed
through a web browser you will notice that such a breakdown is quite popular and natural.

 our application will need to handle requests for:

    posting new tweets(write)
    following a user(write)
    favoriting a tweet(write)
    displaying data about users and tweets(read)


Handling user requests ?
We know that the expected daily load is 100 million requests.
This means that on average the app will receive around 1150 requests per second.




Load balancing
across multiple application instances is a commonly used technique for optimizing resource utilization,
maximizing throughput, reducing latency, and ensuring fault-tolerant configurations.


The following load balancing mechanisms (or methods) are supported in nginx:

    round-robin — requests to the application servers are distributed in a round-robin fashion,
    least-connected — next request is assigned to the server with the least number of active connections,
    ip-hash — a hash-function is used to determine what server should be selected for the next request
             (based on the client’s IP address).


Session persistence

Please note that with round-robin or least-connected load balancing, each subsequent client’s request can be
potentially distributed to a different server. There is no guarantee that the same client will be always
directed to the same server.

If there is the need to tie a client to a particular application server — in other words, make the client’s
session “sticky” or “persistent” in terms of always trying to select a particular server —
the ip-hash load balancing mechanism can be used.

With ip-hash, the client’s IP address is used as a hashing key to determine what server in a server group
should be selected for the client’s requests. This method ensures that the requests from the same client
will always be directed to the same server except when this server is unavailable.


 There are 3 important factors used to measure a load balancer's performance :

    The session rate(depends on CPU)(request goes in)
    This factor is very important, because it directly determines when the load balancer will not be
    able to distribute all the requests it receives. It is mostly dependant on the CPU. Sometimes,
    you will hear about requests/s or hits/s, and they are the same as sessions/s in HTTP/1.0 or HTTP/1.1
    with keep-alive disabled. Requests/s with keep-alive enabled is generally much higher (since it significantly
    reduces system-side work) but is often meaningless for internet-facing deployments since clients
    often open a large amount of connections and do not send many requests per connection on average.
    This factor is measured with varying object sizes, the fastest results generally coming from
    empty objects (eg: HTTP 302, 304 or 404 response codes). Session rates around 100,000 sessions/s
    can be achieved on Xeon E5 systems in 2014.

    The session concurrency(depends on memory)
    This factor is tied to the previous one. Generally, the session rate will drop when the number of
    concurrent sessions increases (except with the epoll or kqueue polling mechanisms). The slower the servers,
    the higher the number of concurrent sessions for a same session rate. If a load balancer receives 10000 sessions
    per second and the servers respond in 100 ms, then the load balancer will have 1000 concurrent sessions.
    This number is limited by the amount of memory and the amount of file-descriptors the system can handle.
    With 16 kB buffers, HAProxy will need about 34 kB per session, which results in around 30000 sessions
    per GB of RAM. In practise, socket buffers in the system also need some memory and 20000 sessions per GB of RAM
    is more reasonable. Layer 4 load balancers generally announce millions of simultaneous sessions
    because they need to deal with the TIME_WAIT sockets that the system handles for free in a proxy.
    Also they don't process any data so they don't need any buffer. Moreover, they are sometimes designed
    to be used in Direct Server Return mode, in which the load balancer only sees forward traffic,
    and which forces it to keep the sessions for a long time after their end to avoid cutting sessions
    before they are closed.

    The data forwarding rate(depend on CPU)(data going out)
    This factor generally is at the opposite of the session rate. It is measured in Megabytes/s (MB/s),
    or sometimes in Gigabits/s (Gbps). Highest data rates are achieved with large objects to minimise the overhead
    caused by session setup and teardown. Large objects generally increase session concurrency, and high session
    concurrency with high data rate requires large amounts of memory to support large windows. High data rates
    burn a lot of CPU and bus cycles on software load balancers because the data has to be copied from the input
    interface to memory and then back to the output device. Hardware load balancers tend to directly switch packets
    from input port to output port for higher data rate, but cannot process them and sometimes fail to touch a header
    or a cookie. Haproxy on a typical Xeon E5 of 2014 can forward data up to about 40 Gbps. A fanless 1.6 GHz Atom CPU
    is slightly above 1 Gbps.


A load balancer's performance related to these factors is generally announced for the best case (eg: empty objects
for session rate, large objects for data rate). This is not because of lack of honesty from the vendors, but because
it is not possible to tell exactly how it will behave in every combination. So when those 3 limits are known,
the customer should be aware that it will generally perform below all of them. A good rule of thumb on software
load balancers is to consider an average practical performance of half of maximal session and data rates for
average sized objects.


Storing the data
We need to store data about our users and their tweets to make the application complete.

After this quick analysis it is obvious that the tweets will take up the majority of our storage’s space.
In general, you don’t need to make very detailed calculations especially if you don’t have much time. However,
it is important to build a rough idea about the size of the data that you will need to handle. If you don’t figure
that out any design decision at the higher or lower level may be inappropriate.


We know that the expected size of the data to store is around 2.6 - 2.7 terabytes.
Real-life examples show that famous companies like Twitter and Facebook manage to use relational databases
for handling much bigger loads than that.



Twitter's original tweet store:

    Temporally sharded tweets was a good-idea-at-the-time architecture.

    Temporal sharding simply means tweets from the same date range are stored together on the same shard.

    The problem is tweets filled up one machine, then a second, and then a third.
    You end up filling up one machine after another.

    This is a pretty common approach and one that has some real flaws:
        Load balancing. Most of the old machines didn't get any traffic because people are interested in
        what is happening now, especially with Twitter.

        Expensive. They filled up one machine, with all its replication slaves, every three weeks,
        which is an expensive setup.

        Logistically complicated. Building a whole new cluster every three weeks is a pain for the DBA team.



In order to handle the incoming read requests we may need to use a caching solution, which stands in front of
the database server. One such popular tool is memcached. It could save us a lot of reads directly from the database.


For example, you just mentioned using a caching solution and imagine that the interviewer asks you:

    “Sounds good, but could you tell me more about why reading from the cache would be better than
    just reading from the database?”

To answer the question asked, we could say that a database stores data on disk and it is much slower to read
from disk than from memory. A solution like memcached stores data in memory, which provides way faster access.
We would need to clarify further that databases usually have their own caching mechanisms but with memcached
we have better control over what gets cached and how. For example, we could store more complex pieces of data
like the results from popular queries.


Going further, in order to make it possible to answer read queries fast we will definitely need to
add the appropriate indexes. This will also be vital for executing quick queries joining tables.
Considering the size of the data we may also think about partitioning the data in some way.
This can improve the read and write speeds and also make administration tasks like backups faster.



Database schema
“If you’re going to use a relational database for storing all the data
could you draft the tables and the relations between them?”


These two entities have several types of relations between them:

    users create tweets
    users can follow users
    users favorite tweets


he first relation is addressed by sticking the user ID to each tweet.
This is possible because each tweet is created by exactly one user.
It’s a bit more complicated when it comes to following users and favoriting tweets.
The relationship there is many-to-many.




Table connections

    ID of user that follows (follower_id)
    ID of user that is followed (followee_id)
    date of creation (created_at)



Table favorites

    ID of user that favorited (user_id)
    ID of favorited tweet (tweet_id)
    date of creation (created_at)



Let’s assume that the queries to our data will be filtering users by their username.
If that’s the case we will definitely want to build an index over this field to optimize the times for such queries.


The next popular query will fetch tweets for a given user.
The query needed for doing that will filter tweets using user_id, which every tweet has.
It makes a lot of sense to build an index over this field in the tweets table,
so that such queries are performed quickly.

We will probably not want to fetch all tweets of a user at once. For example, if a given user has accumulated
several thousand tweets over time, on their profile page we will start by showing the most recent 20 or something
like that. This means that we could use a query, which not only filters by user_id but also orders by creation date
(created_at) and limits the result. Based on that we may think about expanding our index to include the user_id column
but to also include the created_at column. When we have an index over more than one column the order of the columns
matters. If our index looks like that: <user_id, created_at>, making a query filtering by just user_id will
take advantage of the index even though we are not filtering by the second column. So, such an index will allow us
to filter either by just user_id, or by both columns. This will allow us to fetch all tweets authored by a given user
or to isolate just the tweets created in a given time frame. Both will be useful queries for our application.


Having discussed these base use cases, our interviewer suggests that you think about one more possible situation:
“Do you think you could support with our database design the ability to display a page for a given user
with their latest tweets that were favorited at least once?”



Building a RESTful API
Another thing that our interviewer could be interested in is how our front-end would “talk” to the back-end system.

Probably the most popular answer nowadays would be by exposing a RESTful API on the back-end side,
which has a few endpoints returning JSON objects as responses.


GET /api/users/<username>
GET /api/users/<username>/tweets
GET /api/users/<username>/tweets?page=4

GET /api/users/<username>/followers
GET /api/users/<username>/followees

Let’s look at creating new data. For example we will need an endpoint for posting a new tweet:
POST /api/users/<username>/tweets
POST /api/users/<username>/followers


GET /api/users/<username>/tweets/<tweet_id>/favorites
POST /api/users/<username>/tweets/<tweet_id>/favorites


Of course, we will need some sort of authentication to be put in place,
so that we make sure that not everyone can query our exposed API.





Additional Considerations

Increased number of read requests
We have our implementation of the system and it handles everything perfectly.
But what would happen if suddenly we got lucky and people started visiting our application 5 times more often
generating 5 times more read requests caused by viewing posts and user profiles.
What would be the first place that will most likely become a bottleneck?


One very natural answer is our database. It could become overwhelmed with all the read requests coming to it.

One typical way to handle more read requests would be to use replication. This way we could increase the number of
database instances that hold a copy of the data and allow the application to read this data.
Of course, this will help if the write requests are not increased dramatically.


An alternative approach could be to shard our database and spread the data across different machines.
This will help us if we need to write more data than before and the database cannot handle it.
Such an approach does not come for free and it involves other complications that we would need to take care of.
Consider getting familiar with these popular techniques for scaling a relational database.


If we manage to stabilize our database, another point where we could expect problems is the web application itself.
If we’ve been running it on a limited set of machines,
which cannot handle all the load anymore this could lead to slow response times.
One good thing about our high-level design is that it allows us to just add more machines running the application.


This way our load balancer could become a single point of failure and a bottleneck if the number of requests is
really high. In such cases we could start thinking about doing additional load balancing using
DNS and directing requests for our domain to different machines, which are acting as load balancers themselves.



Scaling the database

One approach mentioned above is to add an in-memory cache solution in front of the database with the goal to not
send repeated read requests to the database itself. We could use a key-value store like memcached to handle that.

But this could be insufficient if our data grows too quickly.
In such cases we may have to start thinking about partitioning this data and storing it on separate servers
to increase availability and spread the load.


Sharding our data could be a good and necessary approach in such cases. It must be planned and used with care
because it will cause additional complications. But sometimes it is just required.
It is highly recommended that you read more about this topic and all the implication it brings along.
This will help you justify your decision to shard and to have a discussion about the possible downsides of doing it.



Unexpected traffic
In the beginning the interviewer warned us that there will be outliers in the data.
This means that some users will have many more followers than the average.
Also, some tweets will attract a lot of attention during a short period of time.
In most cases such outliers will generate peaks in the number of requests that our application receives.


As we mentioned earlier this could increase the load on our database.
In such a situation using a caching solution could help a lot.
The idea is that it will be able to answer the popular and repeating requests coming from the application
and these requests will never touch the database, which will be busy replying to other queries.


We could also experience unusual peaks in the requests hitting our application servers.
If they are not enough to respond quickly enough to all requests this could cause timeout to occur for some of
the users. In such situations solutions that offer auto-scaling of the available computing nodes could save the day.
Some companies offering such services are Amazon and Heroku and they were already mentioned in this example.



11. Design the summarization problem
"In our company we already have developed a great library that can be used to summarize text articles.
Just feed it the whole text and it will return a decent summary that is just a few sentences long.

We need to put this in production and make it scalable.
We expect that our customers will submit text articles from our mobile app and also from our website.

The library currently takes between 0.1 and 5 seconds to summarize an article.
You need to design a system that uses our existing library and allows users to submit text articles
through the mobile app and through the website.

We anticipate that this service will be used around 1 million times a month.
Our desire is to not respond in more than 10 seconds to each request."



Given that the library could take up to 5 seconds to respond, looks like we can allow ourselves to have
at most 5 more seconds of additional latency in order not to go over the 10 seconds limit.
This means that it would not be feasible to have a system in which the requests are piled up and processed later.
The processing must be done in real-time with no significant slow down.




However, this information is not quite enough because it would be problematic if a lot of these requests
come in at the same time. Then our service would be overwhelmed to process all incoming requests and it would
take a long time to respond to the ones that came last in the batch. So, it’s good to ask about this, too:
what is the expected maximum number of simultaneous requests?

For this task the answer will be like that:
    "We have some clues, which indicate that we can expect up to 50 requests per second at times.
     But the design should allow us to relatively easily scale up to handle more traffic in the future."



Another question that comes to mind is: do you want to store the results of the processing for a longer period of time?

The answer is:
    "Yes, certainly. We want to store the incoming text articles and the summary for each one of them in order to
     compute various statistics and also to be able to inspect the performance of our algorithms."



Another important question that comes to mind is related to the user experience.
Waiting for 10 seconds on a web site doesn’t sound right. We should clarify what is the expected way of presenting
the results within the website and the mobile app.


The interviewer will then explain:
    "Good point! After our users send a text article through the website or the mobile app,
     we want to redirect them to a screen, which indicates that the summarization is in progress and
     update the screen with the results once they are available."




Here is a summary of what we know so far:

    The expected monthly requests are around 1 million

    There should be at most 50 requests per second but the architecture should easily be expandable
    to handle more than that, if needed

    Responses should not take more than 10 second,
    with the summarization library possibly taking 5 seconds to do its job

    The summaries will be presented to the users asynchronously,
    meaning that we will not wait for the summary to be in the HTTP response that comes after the initial HTTP request





High-level Design
It makes most sense to have one service sitting in the back-end serving requests from all possible front-end clients
instead of having multiple services, one for each type of client. This back-end service will contain the code,
which parses text articles and produces summaries for them by using the existing library.



Now comes the question of how will the front-end clients talk to this back-end service.
One possibility is to just have a RESTful API exposed by the summarization service, which all the front-end clients
send HTTP requests to. This is a rather simple solution but it can have a few problems. We will look at these problems
and discuss a better approach when we start discussing the scalability issues of our architecture.



In order to decide what storage solution is good we should probably know the expected size of the text articles
and the summaries. Hm, seems like we didn’t ask this question initially. It’s ok to come up with additional questions
in the process of building our design. So, let’s ask that.


The interviewer replies:
    "We will limit the size of the text articles to be no more than 100 KB, this is our target at the moment.
     The summaries are meant to be quite smaller than that - no more than 1KB in size."




Low-level Issues

We could make sure that we have a number of instances that are running the summarization service and put a load balancer
in front of them. This will allow us to handle more requests and scale horizontally by adding more instances of the
summarization service. The load balancer will route the requests to the instances that are less loaded.


However, there is another problem with this solution. If for some reason an instance of the summarization service
is not able to handle requests and a request is routed to it, this request will most likely be lost. Such a thing
could happen if all allocated instances are busy with other work. Also, if a given instance is not responding
for some reason, due to a bug in the code, or some other issue,
we could again lose a request that was routed to it by the load balancer.




Let’s try message queues!

In short, a message queue will allow us to enqueue on it all summarization requests.
Let’s call all such requests stored on the queue jobs. Then, if we have a set of workers running
the summarization service, each worker could pull jobs from the queue, one at a time.
Each job can be processed by the worker that picked it up and the results can be stored in the database.


We will need a message queue that allows multiple consumers to pull messages from it because we will want to be able
to start a number of instances running the summarization service. This way we can easily scale up.

Imagine that suddenly the traffic increases. The queue will start to fill up with jobs because the allocated workers
cannot handle the increased number of requests. We could have a monitoring service that would detect this situation
and would spin up additional workers, which will also start pulling from the queue
and will alleviate the load on the existing workers.


Also, with this solution, we know that only properly operating workers will be pulling jobs from the queue.
Hence, jobs will not be lost due to not responding workers.



It is still possible that a worker takes a job but crashes while processing it.
For example, if we have a bug in the summarization code and it crashes in the middle of the operation.
This means that a job was pulled from the queue but a result was never computed.
Does this mean that the job was lost? Not necessarily.


We will need to make sure that our message queue requires an acknowledgement that the job was successfully processed.
This means that each worker will have to let the queue know that the job that it pulled was processed.
If the worker crashes it won’t notify the queue. After a given timeout passes the queue will assume that the job
was not processed successfully and will make it available for pulling.
This can be done a number of times until the job is discarded to some other place.




As an example, the SQS service offered in AWS, has a default number of retries, currently set to 10.
If a job is pulled and not acknowledged 10 times it can be moved to a special queue called dead-letter queue.
If such a queue is set up, it can be monitored for failed jobs. It will hold such jobs a given number of days,
so that action can be taken on them.




The important point is that using a message queue we achieve two useful things:

    We are ready to scale up easily by spinning up more summarization workers
    We can handle unexpected problems with jobs processing without losing jobs















How to check similarities of two large file?




Picking the right architecture = Picking the right battles + Managing trade-offs

Basic Steps

    Clarify and agree on the scope of the system

    User cases (description of sequences of events that, taken together, lead to a system doing something useful)
        Who is going to use it?
        How are they going to use it?
    Constraints
        Mainly identify traffic and data handling constraints at scale.
        Scale of the system such as requests per second, requests types, data written per second, data read per second)
        Special system requirements such as multi-threading, read or write oriented.

    High level architecture design (Abstract design)

    Sketch the important components and connections between them, but don't go into some details.
        Application service layer (serves the requests)
        List different services required.
        Data Storage layer
        eg. Usually a scalable system includes webserver (load balancer), service (service partition),
        database (master/slave database cluster) and caching systems.

    Component Design

    Component + specific APIs required for each of them.
    Object oriented design for functionalities.
        Map features to modules: One scenario for one module.
        Consider the relationships among modules:
            Certain functions must have unique instance (Singletons)
            Core object can be made up of many other objects (composition).
            One object is another object (inheritance)
    Database schema design.

    Understanding Bottlenecks

    Perhaps your system needs a load balancer and many machines behind it to handle the user requests.
    * Or maybe the data is so huge that you need to distribute your database on multiple machines.
    What are some of the downsides that occur from doing that?

    Is the database too slow and does it need some in-memory caching?

    Scaling your abstract design

    Vertical scaling
        You scale by adding more power (CPU, RAM) to your existing machine.
    Horizontal scaling
        You scale by adding more machines into your pool of resources.
    Caching
        Load balancing helps you scale horizontally across an ever-increasing number of servers,
        but caching will enable you to make vastly better use of the resources you already have,
        as well as making otherwise unattainable product requirements feasible.

        Application caching requires explicit integration in the application code itself.
        Usually it will check if a value is in the cache; if not, retrieve the value from the database.

        Database caching tends to be "free". When you flip your database on, you're going to get some level of default
        configuration which will provide some degree of caching and performance.
        Those initial settings will be optimized for a generic usecase, and by tweaking them to your system's
        access patterns you can generally squeeze a great deal of performance improvement.

        In-memory caches are most potent in terms of raw performance. This is because they store their
        entire set of data in memory and accesses to RAM are orders of magnitude faster
        than those to disk. eg. Memcached or Redis.
        eg. Precalculating results (e.g. the number of visits from each referring domain for the previous day),
        eg. Pre-generating expensive indexes (e.g. suggested stories based on a user's click history)
        eg. Storing copies of frequently accessed data in a faster backend (e.g. Memcache instead of PostgreSQL.
    Load balancing
        Public servers of a scalable web service are hidden behind a load balancer.
        This load balancer evenly distributes load (requests from your users)
        onto your group/cluster of application servers.

        Types: Smart client (hard to get it perfect), Hardware load balancers ($$$ but reliable),
        Software load balancers (hybrid - works for most systems)

Load Balancing

    Database replication
        Database replication is the frequent electronic copying data from a database in one computer or server
        to a database in another so that all users share the same level of information.
        The result is a distributed database in which users can access data relevant to their tasks without
        interfering with the work of others. The implementation of database replication for the purpose
        of eliminating data ambiguity or inconsistency among users is known as normalization.
    Database partitioning
        Partitioning of relational data usually refers to decomposing your tables
        either row-wise (horizontally) or column-wise (vertically).
    Map-Reduce
        For sufficiently small systems you can often get away with adhoc queries on a SQL database,
        but that approach may not scale up trivially once the quantity of data stored or write-load
        requires sharding your database, and will usually require dedicated slaves for the purpose of performing
        these queries (at which point, maybe you'd rather use a system designed for analyzing large quantities
        of data, rather than fighting your database).
        Adding a map-reduce layer makes it possible to perform data and/or processing intensive operations
        in a reasonable amount of time. You might use it for calculating suggested users in a social graph,
        or for generating analytics reports. eg. Hadoop, and maybe Hive or HBase.
    Platform Layer (Services)
        Separating the platform and web application allow you to scale the pieces independently.
        If you add a new API, you can add platform servers
        without adding unnecessary capacity for your web application tier.
        Adding a platform layer can be a way to reuse your infrastructure for multiple products or
        interfaces (a web application, an API, an iPhone app, etc) without writing too much redundant
        boilerplate code for dealing with caches, databases, etc.

Platform Layer
Key topics for designing a system

    Concurrency
    Do you understand threads, deadlock, and starvation? Do you know how to parallelize algorithms?
    Do you understand consistency and coherence?

    Networking
    Do you roughly understand IPC and TCP/IP? Do you know the difference between throughput and latency,
    and when each is the relevant factor?

    Abstraction
    You should understand the systems you’re building upon. Do you know roughly how an OS, file system,
    and database work? Do you know about the various levels of caching in a modern OS?

    Real-World Performance
    You should be familiar with the speed of everything your computer can do,
    including the relative performance of RAM, disk, SSD and your network.

    Estimation
    Estimation, especially in the form of a back-of-the-envelope calculation,
    is important because it helps you narrow down the list of possible solutions to only the ones that are feasible.
    Then you have only a few prototypes or micro-benchmarks to write.

    Availability & Reliability
    Are you thinking about how things can fail, especially in a distributed environment?
    Do know how to design a system to cope with network failures? Do you understand durability?


Web App System design considerations:

    Security (CORS)
    Using CDN
        A content delivery network (CDN) is a system of distributed servers (network) that deliver webpages
        and other Web content to a user based on the geographic locations of the user,
        the origin of the webpage and a content delivery server.
        This service is effective in speeding the delivery of content of websites with high traffic
        and websites that have global reach. The closer the CDN server is to the user geographically,
        the faster the content will be delivered to the user.
        CDNs also provide protection from large surges in traffic.
    Full Text Search
        Using Sphinx/Lucene/Solr - which achieve fast search responses because, instead of searching the text directly,
        it searches an index instead.
    Offline support/Progressive enhancement
        Service Workers
    Web Workers
    Server Side rendering
    Asynchronous loading of assets (Lazy load items)
    Minimizing netwrok requests (Http2 + bundling/sprites etc)
    Developer productivity/Tooling
    Accessibility
    Internationalization
    Responsive design
    Browser compatibility

Working Components of Front-end Architecture

    Code
        HTML5/WAI-ARIA
        CSS/Sass Code standards and organization
        Object-Oriented approach (how do objects break down and get put together)
        JS frameworks/organization/performance optimization techniques
        Asset Delivery - Front-end Ops
    Documentation
        Onboarding Docs
        Styleguide/Pattern Library
        Architecture Diagrams (code flow, tool chain)
    Testing
        Performance Testing
        Visual Regression
        Unit Testing
        End-to-End Testing
    Process
        Git Workflow
        Dependency Management (npm, Bundler, Bower)
        Build Systems (Grunt/Gulp)
        Deploy Process
        Continuous Integration (Travis CI, Jenkins)



Links
How to rock a systems design interview

System Design Interviewing

Scalability for Dummies

Introduction to Architecting Systems for Scale

Scalable System Design Patterns

Scalable Web Architecture and Distributed Systems

What is the best way to design a web site to be highly scalable?

How web works?


https://gist.github.com/zerghua/941fec17c95547ef15af16bb7abd88dd




Data Partitioning(sharding)
Data partitioning (aka. sharding) is a technique to break up a big database (DB) into many smaller parts.
It is the process of splitting up a DB/table across multiple machines to improve the manageability, performance,
availability and load balancing of an application.


1. Partitioning Methods

a. Horizontal partitioning: In this scheme, we put different rows into different tables.
For example, if we are storing different places in a table, we can decide that locations with ZIP codes
less than 10000 are stored in one table, and places with ZIP codes greater than 10000 are stored in a separate table.
This is also called a range based sharding, as we are storing different ranges of data in separate tables.

The key problem with this approach is that if the value whose range is used for sharding isn’t chosen carefully,
then the partitioning scheme will lead to unbalanced servers. In the previous example, splitting location
based on their zip codes assumes that places will be evenly distributed across the different zip codes.
This assumption is not valid as there will be a lot of places in a thickly populated area like Manhattan
compared to its suburb cities.


b. Vertical Partitioning: In this scheme, we divide our data to store tables related to a specific feature
to their own server. For example, if we are building Instagram like application, where we need to store data
related to users, all the photos they upload and people they follow, we can decide to place user profile information
on one DB server, friend lists on another and photos on a third server.

Vertical partitioning is straightforward to implement and has a low impact on the application.
The main problem with this approach is that if our application experiences additional growth,
then it may be necessary to further partition a feature specific DB across various servers
(e.g. it would not be possible for a single server to handle all the metadata queries for 10 billion photos
by 140 million users).



c. Directory Based Partitioning: A loosely coupled approach to work around issues mentioned in above schemes
is to create a lookup service which knows your current partitioning scheme and abstracts it away from
the DB access code. So, to find out where does a particular data entity resides, we query our directory server
that holds the mapping between each tuple key to its DB server. This loosely coupled approach means we can perform
tasks like adding servers to the DB pool or change our partitioning scheme without having to impact your application.




2. Partitioning Criteria
a. Key or Hash-based partitioning: Under this scheme, we apply a hash function to some key attribute of the entity
we are storing, that yields the partition number. For example, if we have 100 DB servers and our ID is a numeric
value that gets incremented by one, each time a new record is inserted. In this example, the hash function could
be ‘ID % 100’, which will give us the server number where we can store/read that record.
This approach should ensure a uniform allocation of data among servers.
The fundamental problem with this approach is that it effectively fixes the total number of DB servers,
since adding new servers means changing the hash function which would require redistribution of data and downtime
for the service.
A workaround for this problem is to use Consistent Hashing.

Consistent Hashing?




b. List partitioning: In this scheme, each partition is assigned a list of values,
so whenever we want to insert a new record, we will see which partition contains our key and then store it there.
For example, we can decide all users living in Iceland, Norway, Sweden, Finland or Denmark will be stored in a
partition for the Nordic countries.


c. Round-robin partitioning: This is a very simple strategy that ensures uniform data distribution.
With ‘n’ partitions, the ‘i’ tuple is assigned to partition (i mod n).


d. Composite partitioning: Under this scheme, we combine any of above partitioning schemes to devise a new scheme.
For example, first applying a list partitioning and then a hash based partitioning.
Consistent hashing could be considered a composite of hash and list partitioning
where the hash reduces the key space to a size that can be listed.



3. Common Problems of Sharding
a. Joins and Denormalization: Performing joins on a database which is running on one server is straightforward,
but once a database is partitioned and spread across multiple machines
it is often not feasible to perform joins that span database shards.


b. Referential integrity: As we saw that performing a cross-shard query on a partitioned database is not feasible,
similarly trying to enforce data integrity constraints such as foreign keys
in a sharded database can be extremely difficult.


c. Rebalancing: There could be many reasons we have to change our sharding scheme:
    The data distribution is not uniform,
        e.g., there are a lot of places for a particular ZIP code, that cannot fit into one database partition.

    There are a lot of load on a shard,
        e.g., there are too many requests being handled by the DB shard dedicated to user photos.




Cache Invalidation
While caching is fantastic, it does require some maintenance for keeping cache coherent with the
source of truth (e.g., database). If the data is modified in the database, it should be invalidated in the cache,
if not, this can cause inconsistent application behavior.

Solving this problem is known as cache invalidation, there are three main schemes that are used:

1. Write-through cache:
Under this scheme data is written into the cache and the corresponding database at the same time.
Although write through minimizes the risk of data loss, since every write operation must be done twice
before returning success to the client, this scheme has the disadvantage of higher latency for write operations.


2. Write-around cache:
This technique is similar to write through cache, but data is written directly to permanent storage,
bypassing the cache.
This can reduce the cache being flooded with write operations that will not subsequently be re-read,
but has the disadvantage that a read request for recently written data will create a “cache miss” and
must be read from slower back-end storage and experience higher latency.


3. Write-back cache:
Under this scheme, data is written to cache alone, and completion is immediately confirmed to the client.
The write to the permanent storage is done after specified intervals or under certain conditions.
This results in low latency and high throughput for write-intensive applications, however,
this speed comes with the risk of data loss in case of a crash or other adverse event
because the only copy of the written data is in the cache.



