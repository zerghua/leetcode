1. Design POI(point of interest)



2. Design Instagram




3. Design memcached



4. Design display top 10 hits music




5. OOD: design a vending machine.




6. web server: how to handle request, TCP connection limitation,
   if TCP were turned off, why server were not shutdown immediately




7. Design server to host music, how to download music, update music, recommend music




8. Design Rate limiter
https://github.com/google/guava/blob/9b94fb3965c6869b0ac47420958a4bbae0b2d54c/android/guava/src/com/google/common/util/concurrent/SmoothRateLimiter.java


9. Design Hit Counter
http://blog.gainlo.co/index.php/2016/09/12/dropbox-interview-design-hit-counter/?utm_source=glassdoor&utm_campaign=glassdoor&utm_medium=92116
It starts with a simple question – if you are building a website, how do you count the number of visitors for the past 1 minute?


10. Design a simplified version of Twitter where people can post tweets, follow other people and favorite tweets.

Q1: First of all, how many users do we expect this system to handle?
A1: “well… to make things interesting, let’s aim for 10 million users generating around 100 million requests per day”


Q2: how connected will these users be?(they will form a graph with the users being the nodes and the edges will represent who follows whom. )
A2: “we expect that each user will be following 200 other users on average,
but expect some extraordinary users with tens of thousands of followers”


Q3: We have another dimension in the application: tweets.
Producing new tweets and favoriting them should be the most common write operations in the application.
But how many requests will that generate?
A3: “Hm, that’s hard to say… we expect that there will be a maximum of 10 million tweets per day and
each tweet will probably be favorited twice on average but again, expect some big outliers.”


Let’s make a few very simple calculations with the information we just received.
We will have around 10 million users. Average number of followed other users is 200.
This means that the network of users will have about 200 * 10 million edges. This makes 2 billion edges.
If the average number of tweets per day is 10 million the number of favorites will then be 20 million.


As it is often the case, we can divide our architecture in two logical parts:
1) the logic, which will handle all incoming requests to the application and
2) the data storage that we will use to store all the data that needs to be persisted.
If you learn enough about popular software systems, especially the ones that are accessed
through a web browser you will notice that such a breakdown is quite popular and natural.

 our application will need to handle requests for:

    posting new tweets(write)
    following a user(write)
    favoriting a tweet(write)
    displaying data about users and tweets(read)


Handling user requests ?
We know that the expected daily load is 100 million requests.
This means that on average the app will receive around 1150 requests per second.




Load balancing
across multiple application instances is a commonly used technique for optimizing resource utilization,
maximizing throughput, reducing latency, and ensuring fault-tolerant configurations.


The following load balancing mechanisms (or methods) are supported in nginx:

    round-robin — requests to the application servers are distributed in a round-robin fashion,
    least-connected — next request is assigned to the server with the least number of active connections,
    ip-hash — a hash-function is used to determine what server should be selected for the next request
             (based on the client’s IP address).


Session persistence

Please note that with round-robin or least-connected load balancing, each subsequent client’s request can be
potentially distributed to a different server. There is no guarantee that the same client will be always
directed to the same server.

If there is the need to tie a client to a particular application server — in other words, make the client’s
session “sticky” or “persistent” in terms of always trying to select a particular server —
the ip-hash load balancing mechanism can be used.

With ip-hash, the client’s IP address is used as a hashing key to determine what server in a server group
should be selected for the client’s requests. This method ensures that the requests from the same client
will always be directed to the same server except when this server is unavailable.


 There are 3 important factors used to measure a load balancer's performance :

    The session rate(depends on CPU)(request goes in)
    This factor is very important, because it directly determines when the load balancer will not be
    able to distribute all the requests it receives. It is mostly dependant on the CPU. Sometimes,
    you will hear about requests/s or hits/s, and they are the same as sessions/s in HTTP/1.0 or HTTP/1.1
    with keep-alive disabled. Requests/s with keep-alive enabled is generally much higher (since it significantly
    reduces system-side work) but is often meaningless for internet-facing deployments since clients
    often open a large amount of connections and do not send many requests per connection on average.
    This factor is measured with varying object sizes, the fastest results generally coming from
    empty objects (eg: HTTP 302, 304 or 404 response codes). Session rates around 100,000 sessions/s
    can be achieved on Xeon E5 systems in 2014.

    The session concurrency(depends on memory)
    This factor is tied to the previous one. Generally, the session rate will drop when the number of
    concurrent sessions increases (except with the epoll or kqueue polling mechanisms). The slower the servers,
    the higher the number of concurrent sessions for a same session rate. If a load balancer receives 10000 sessions
    per second and the servers respond in 100 ms, then the load balancer will have 1000 concurrent sessions.
    This number is limited by the amount of memory and the amount of file-descriptors the system can handle.
    With 16 kB buffers, HAProxy will need about 34 kB per session, which results in around 30000 sessions
    per GB of RAM. In practise, socket buffers in the system also need some memory and 20000 sessions per GB of RAM
    is more reasonable. Layer 4 load balancers generally announce millions of simultaneous sessions
    because they need to deal with the TIME_WAIT sockets that the system handles for free in a proxy.
    Also they don't process any data so they don't need any buffer. Moreover, they are sometimes designed
    to be used in Direct Server Return mode, in which the load balancer only sees forward traffic,
    and which forces it to keep the sessions for a long time after their end to avoid cutting sessions
    before they are closed.

    The data forwarding rate(depend on CPU)(data going out)
    This factor generally is at the opposite of the session rate. It is measured in Megabytes/s (MB/s),
    or sometimes in Gigabits/s (Gbps). Highest data rates are achieved with large objects to minimise the overhead
    caused by session setup and teardown. Large objects generally increase session concurrency, and high session
    concurrency with high data rate requires large amounts of memory to support large windows. High data rates
    burn a lot of CPU and bus cycles on software load balancers because the data has to be copied from the input
    interface to memory and then back to the output device. Hardware load balancers tend to directly switch packets
    from input port to output port for higher data rate, but cannot process them and sometimes fail to touch a header
    or a cookie. Haproxy on a typical Xeon E5 of 2014 can forward data up to about 40 Gbps. A fanless 1.6 GHz Atom CPU
    is slightly above 1 Gbps.


A load balancer's performance related to these factors is generally announced for the best case (eg: empty objects
for session rate, large objects for data rate). This is not because of lack of honesty from the vendors, but because
it is not possible to tell exactly how it will behave in every combination. So when those 3 limits are known,
the customer should be aware that it will generally perform below all of them. A good rule of thumb on software
load balancers is to consider an average practical performance of half of maximal session and data rates for
average sized objects.




Picking the right architecture = Picking the right battles + Managing trade-offs

Basic Steps

    Clarify and agree on the scope of the system

    User cases (description of sequences of events that, taken together, lead to a system doing something useful)
        Who is going to use it?
        How are they going to use it?
    Constraints
        Mainly identify traffic and data handling constraints at scale.
        Scale of the system such as requests per second, requests types, data written per second, data read per second)
        Special system requirements such as multi-threading, read or write oriented.

    High level architecture design (Abstract design)

    Sketch the important components and connections between them, but don't go into some details.
        Application service layer (serves the requests)
        List different services required.
        Data Storage layer
        eg. Usually a scalable system includes webserver (load balancer), service (service partition),
        database (master/slave database cluster) and caching systems.

    Component Design

    Component + specific APIs required for each of them.
    Object oriented design for functionalities.
        Map features to modules: One scenario for one module.
        Consider the relationships among modules:
            Certain functions must have unique instance (Singletons)
            Core object can be made up of many other objects (composition).
            One object is another object (inheritance)
    Database schema design.

    Understanding Bottlenecks

    Perhaps your system needs a load balancer and many machines behind it to handle the user requests.
    * Or maybe the data is so huge that you need to distribute your database on multiple machines.
    What are some of the downsides that occur from doing that?

    Is the database too slow and does it need some in-memory caching?

    Scaling your abstract design

    Vertical scaling
        You scale by adding more power (CPU, RAM) to your existing machine.
    Horizontal scaling
        You scale by adding more machines into your pool of resources.
    Caching
        Load balancing helps you scale horizontally across an ever-increasing number of servers,
        but caching will enable you to make vastly better use of the resources you already have,
        as well as making otherwise unattainable product requirements feasible.

        Application caching requires explicit integration in the application code itself.
        Usually it will check if a value is in the cache; if not, retrieve the value from the database.

        Database caching tends to be "free". When you flip your database on, you're going to get some level of default
        configuration which will provide some degree of caching and performance.
        Those initial settings will be optimized for a generic usecase, and by tweaking them to your system's
        access patterns you can generally squeeze a great deal of performance improvement.

        In-memory caches are most potent in terms of raw performance. This is because they store their
        entire set of data in memory and accesses to RAM are orders of magnitude faster
        than those to disk. eg. Memcached or Redis.
        eg. Precalculating results (e.g. the number of visits from each referring domain for the previous day),
        eg. Pre-generating expensive indexes (e.g. suggested stories based on a user's click history)
        eg. Storing copies of frequently accessed data in a faster backend (e.g. Memcache instead of PostgreSQL.
    Load balancing
        Public servers of a scalable web service are hidden behind a load balancer.
        This load balancer evenly distributes load (requests from your users)
        onto your group/cluster of application servers.

        Types: Smart client (hard to get it perfect), Hardware load balancers ($$$ but reliable),
        Software load balancers (hybrid - works for most systems)

Load Balancing

    Database replication
        Database replication is the frequent electronic copying data from a database in one computer or server
        to a database in another so that all users share the same level of information.
        The result is a distributed database in which users can access data relevant to their tasks without
        interfering with the work of others. The implementation of database replication for the purpose
        of eliminating data ambiguity or inconsistency among users is known as normalization.
    Database partitioning
        Partitioning of relational data usually refers to decomposing your tables
        either row-wise (horizontally) or column-wise (vertically).
    Map-Reduce
        For sufficiently small systems you can often get away with adhoc queries on a SQL database,
        but that approach may not scale up trivially once the quantity of data stored or write-load
        requires sharding your database, and will usually require dedicated slaves for the purpose of performing
        these queries (at which point, maybe you'd rather use a system designed for analyzing large quantities
        of data, rather than fighting your database).
        Adding a map-reduce layer makes it possible to perform data and/or processing intensive operations
        in a reasonable amount of time. You might use it for calculating suggested users in a social graph,
        or for generating analytics reports. eg. Hadoop, and maybe Hive or HBase.
    Platform Layer (Services)
        Separating the platform and web application allow you to scale the pieces independently.
        If you add a new API, you can add platform servers
        without adding unnecessary capacity for your web application tier.
        Adding a platform layer can be a way to reuse your infrastructure for multiple products or
        interfaces (a web application, an API, an iPhone app, etc) without writing too much redundant
        boilerplate code for dealing with caches, databases, etc.

Platform Layer
Key topics for designing a system

    Concurrency
    Do you understand threads, deadlock, and starvation? Do you know how to parallelize algorithms?
    Do you understand consistency and coherence?

    Networking
    Do you roughly understand IPC and TCP/IP? Do you know the difference between throughput and latency,
    and when each is the relevant factor?

    Abstraction
    You should understand the systems you’re building upon. Do you know roughly how an OS, file system,
    and database work? Do you know about the various levels of caching in a modern OS?

    Real-World Performance
    You should be familiar with the speed of everything your computer can do,
    including the relative performance of RAM, disk, SSD and your network.

    Estimation
    Estimation, especially in the form of a back-of-the-envelope calculation,
    is important because it helps you narrow down the list of possible solutions to only the ones that are feasible.
    Then you have only a few prototypes or micro-benchmarks to write.

    Availability & Reliability
    Are you thinking about how things can fail, especially in a distributed environment?
    Do know how to design a system to cope with network failures? Do you understand durability?


Web App System design considerations:

    Security (CORS)
    Using CDN
        A content delivery network (CDN) is a system of distributed servers (network) that deliver webpages
        and other Web content to a user based on the geographic locations of the user,
        the origin of the webpage and a content delivery server.
        This service is effective in speeding the delivery of content of websites with high traffic
        and websites that have global reach. The closer the CDN server is to the user geographically,
        the faster the content will be delivered to the user.
        CDNs also provide protection from large surges in traffic.
    Full Text Search
        Using Sphinx/Lucene/Solr - which achieve fast search responses because, instead of searching the text directly,
        it searches an index instead.
    Offline support/Progressive enhancement
        Service Workers
    Web Workers
    Server Side rendering
    Asynchronous loading of assets (Lazy load items)
    Minimizing netwrok requests (Http2 + bundling/sprites etc)
    Developer productivity/Tooling
    Accessibility
    Internationalization
    Responsive design
    Browser compatibility

Working Components of Front-end Architecture

    Code
        HTML5/WAI-ARIA
        CSS/Sass Code standards and organization
        Object-Oriented approach (how do objects break down and get put together)
        JS frameworks/organization/performance optimization techniques
        Asset Delivery - Front-end Ops
    Documentation
        Onboarding Docs
        Styleguide/Pattern Library
        Architecture Diagrams (code flow, tool chain)
    Testing
        Performance Testing
        Visual Regression
        Unit Testing
        End-to-End Testing
    Process
        Git Workflow
        Dependency Management (npm, Bundler, Bower)
        Build Systems (Grunt/Gulp)
        Deploy Process
        Continuous Integration (Travis CI, Jenkins)



Links
How to rock a systems design interview

System Design Interviewing

Scalability for Dummies

Introduction to Architecting Systems for Scale

Scalable System Design Patterns

Scalable Web Architecture and Distributed Systems

What is the best way to design a web site to be highly scalable?

How web works?


https://gist.github.com/zerghua/941fec17c95547ef15af16bb7abd88dd